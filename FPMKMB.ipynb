{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "tokenTwitter = '44dbcc46f990445db65fab0d47adf939ef3d8695'\n",
        "\n",
        "config = {\n",
        "    'namaFile': 'pendakian.csv',\n",
        "    'kataKunci': 'pendakian since:2018-01-01 until:2025-12-01 lang:id',\n",
        "    'limitTweets': 500,\n",
        "    'tab': 'LATEST'\n",
        "}\n",
        "\n",
        "def installPkg():\n",
        "    subprocess.run(['apt-get', 'update'], check=False)\n",
        "\n",
        "    daftarPkg = [\n",
        "        'ca-certificates', 'curl', 'gnupg',\n",
        "        'libatk1.0-0', 'libatk-bridge2.0-0', 'libcups2',\n",
        "        'libxcomposite1', 'libxdamage1', 'libxfixes3',\n",
        "        'libxrandr2', 'libgbm1', 'libpango-1.0-0',\n",
        "        'libcairo2', 'libasound2'\n",
        "    ]\n",
        "\n",
        "    for pkg in daftarPkg:\n",
        "        subprocess.run(['apt-get', 'install', '-y', pkg], check=False)\n",
        "\n",
        "def installNode():\n",
        "    try:\n",
        "        perintah = [\n",
        "            ['mkdir', '-p', '/etc/apt/keyrings'],\n",
        "            ['bash', '-c', 'curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg'],\n",
        "            ['bash', '-c', 'echo \"deb [signed-by=//etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main\" | tee /etc/apt/sources.list.d/nodesource.list'],\n",
        "            ['apt-get', 'update'],\n",
        "            ['apt-get', 'install', 'nodejs', '-y']\n",
        "        ]\n",
        "\n",
        "        for cmd in perintah:\n",
        "            subprocess.run(cmd, check=False)\n",
        "\n",
        "        hasil = subprocess.run(['node', '-v'], capture_output=True, text=True)\n",
        "        print(f\"Node.js Terinstall: {hasil.stdout.strip()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error Instal Node.js: {e}\")\n",
        "\n",
        "def installPlaywright():\n",
        "    try:\n",
        "        subprocess.run(['npm', 'uninstall', '-g', 'playwright'], check=False)\n",
        "        subprocess.run(['npm', 'install', '-g', 'playwright'], check=False)\n",
        "        subprocess.run(['playwright', 'install', 'chromium'], check=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error Instal Playwright: {e}\")\n",
        "\n",
        "def ambilTweet():\n",
        "    print(f\"Mulai Ambil Tweet...\")\n",
        "    print(f\"Kata Kunci: {config['kataKunci']}\")\n",
        "    print(f\"Jumlah: {config['limitTweets']}\")\n",
        "    print(f\"File Output: {config['namaFile']}\")\n",
        "\n",
        "    perintah = [\n",
        "        'npx', '-y', 'tweet-harvest@2.6.1',\n",
        "        '-o', config['namaFile'],\n",
        "        '-s', config['kataKunci'],\n",
        "        '--tab', config['tab'],\n",
        "        '-l', str(config['limitTweets']),\n",
        "        '--token', tokenTwitter\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        hasil = subprocess.run(perintah, capture_output=True, text=True)\n",
        "\n",
        "        if hasil.returncode == 0:\n",
        "            print(\"Ambil Tweet Selesai.\")\n",
        "        else:\n",
        "            print(f\"Gagal Ambil Tweet: {hasil.stderr}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error Tweet-Harvest: {e}\")\n",
        "\n",
        "def main():\n",
        "    print(\"PROSES AMBIL TWEET\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    installPkg()\n",
        "    installNode()\n",
        "    installPlaywright()\n",
        "    ambilTweet()\n",
        "\n",
        "    print(\"\\nSELESAI!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "N-IzycJx_0Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/temp_group_1.csv')\n",
        "\n",
        "df['text'] = df['full_text']\n",
        "df['label'] = 1\n",
        "\n",
        "new_df = df[['text', 'label']]\n",
        "\n",
        "new_df.to_csv('dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "x_cTQ87-6JNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'RT\\s+', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df = pd.read_csv('new_file.csv')\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "df.to_csv('cleaned_file.csv', index=False)"
      ],
      "metadata": {
        "id": "A6fVCiva6Wff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "classifier = None\n",
        "vectorizer = None\n",
        "\n",
        "def load_data(filename):\n",
        "    global classifier, vectorizer\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    required_cols = ['text', 'label']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(\"Error: CSV harus punya kolom 'text' dan 'label'\")\n",
        "        return None\n",
        "\n",
        "    X = df['text']\n",
        "    y = df['label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "    y_pred = classifier.predict(X_test_vec)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Akurasi model: {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    model_data = {\n",
        "        'model': classifier,\n",
        "        'vectorizer': vectorizer\n",
        "    }\n",
        "    joblib.dump(model_data, 'tweet_classifier_model.pkl')\n",
        "    print(\"Model disimpan sebagai 'tweet_classifier_model.pkl'\")\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def main():\n",
        "    filename = input(\"Masukkan nama file CSV dataset: \").strip()\n",
        "\n",
        "    try:\n",
        "        model = load_data(filename)\n",
        "        if model is not None:\n",
        "            print(\"Model berhasil dibuat dan disimpan.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{filename}' tidak ditemukan\")\n",
        "    except Exception as e:\n",
        "        print(f\"Terjadi error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-SwmiLDM6ive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_model():\n",
        "    try:\n",
        "        model_data = joblib.load('tweet_classifier_model.pkl')\n",
        "        classifier = model_data['model']\n",
        "        vectorizer = model_data['vectorizer']\n",
        "        print(\"Model berhasil dimuat\")\n",
        "        return classifier, vectorizer\n",
        "    except:\n",
        "        print(\"Model 'tweet_classifier_model.pkl' tidak ditemukan\")\n",
        "        print(\"   Silakan train model terlebih dahulu\")\n",
        "        return None, None\n",
        "\n",
        "def predict_single_tweet(classifier, vectorizer, tweet):\n",
        "    tweet_vec = vectorizer.transform([tweet])\n",
        "    prediction = classifier.predict(tweet_vec)[0]\n",
        "    probability = classifier.predict_proba(tweet_vec)[0]\n",
        "\n",
        "    if prediction == 1:\n",
        "        label = \"GUNUNG MELETUS\"\n",
        "    else:\n",
        "        label = \"BUKAN gunung meletus\"\n",
        "\n",
        "    prob_volcano = probability[1]\n",
        "    prob_not = probability[0]\n",
        "\n",
        "    print(\"\\nHASIL PREDIKSI:\")\n",
        "    print(\"   Tweet:\", tweet)\n",
        "    print(\"   Kategori:\", label)\n",
        "    print(\"   Probabilitas gunung meletus:\", f\"{prob_volcano:.2%}\")\n",
        "    print(\"   Probabilitas bukan:\", f\"{prob_not:.2%}\")\n",
        "\n",
        "    return prediction, prob_volcano\n",
        "\n",
        "def predict_from_csv(classifier, vectorizer, csv_file):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        if 'text' not in df.columns:\n",
        "            print(\"File CSV harus memiliki kolom 'text'\")\n",
        "            return\n",
        "\n",
        "        tweets = df['text'].tolist()\n",
        "        tweets_vec = vectorizer.transform(tweets)\n",
        "\n",
        "        predictions = classifier.predict(tweets_vec)\n",
        "        probabilities = classifier.predict_proba(tweets_vec)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            'text': tweets,\n",
        "            'prediction': predictions,\n",
        "            'category': ['GUNUNG MELETUS' if p == 1 else 'BUKAN' for p in predictions],\n",
        "            'prob_gunung_meletus': probabilities[:, 1],\n",
        "            'prob_bukan': probabilities[:, 0]\n",
        "        })\n",
        "\n",
        "        print(\"\\nHASIL PREDIKSI DARI FILE:\", csv_file)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for idx, row in results.iterrows():\n",
        "            print(f\"\\nTweet {idx+1}: {row['text'][:100]}...\")\n",
        "            print(\"  Kategori:\", row['category'])\n",
        "            print(\"  Probabilitas gunung meletus:\", f\"{row['prob_gunung_meletus']:.2%}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"STATISTIK:\")\n",
        "        volcano_count = (results['prediction'] == 1).sum()\n",
        "        total_count = len(results)\n",
        "        print(\"   Jumlah tweet:\", total_count)\n",
        "        print(\"   Gunung meletus:\", volcano_count, \"tweet\")\n",
        "        print(\"   Bukan:\", total_count - volcano_count, \"tweet\")\n",
        "\n",
        "        results.to_csv('hasil_prediksi.csv', index=False)\n",
        "        print(\"\\nHasil prediksi disimpan sebagai 'hasil_prediksi.csv'\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File\", csv_file, \"tidak ditemukan\")\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", str(e))\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SISTEM PREDIKSI TWEET GUNUNG MELETUS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    classifier, vectorizer = load_model()\n",
        "\n",
        "    if classifier is None or vectorizer is None:\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"-\" * 40)\n",
        "        print(\"MENU PREDIKSI:\")\n",
        "        print(\"1. Input tweet manual\")\n",
        "        print(\"2. Upload file CSV\")\n",
        "        print(\"3. Keluar\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        choice = input(\"Pilih menu (1/2/3): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            print(\"\\nINPUT TWEET MANUAL\")\n",
        "            print(\"=\" * 40)\n",
        "            tweet = input(\"Masukkan teks tweet: \").strip()\n",
        "            if tweet:\n",
        "                predict_single_tweet(classifier, vectorizer, tweet)\n",
        "            else:\n",
        "                print(\"Tweet tidak boleh kosong\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"\\nUPLOAD FILE CSV\")\n",
        "            print(\"=\" * 40)\n",
        "            print(\"Format file CSV harus memiliki kolom 'text'\")\n",
        "            print(\"Contoh struktur file:\")\n",
        "            print(\"text\")\n",
        "            print(\"\\\"gunung merapi erupsi malam ini\\\"\")\n",
        "            print(\"\\\"hari ini cuaca cerah\\\"\")\n",
        "            print(\"=\" * 40)\n",
        "\n",
        "            csv_file = input(\"Masukkan nama file CSV: \").strip()\n",
        "            if csv_file:\n",
        "                predict_from_csv(classifier, vectorizer, csv_file)\n",
        "            else:\n",
        "                print(\"Nama file tidak boleh kosong\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"\\nProgram selesai\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Pilihan tidak valid. Silakan pilih 1, 2, atau 3.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rK-6Oan6-2Y",
        "outputId": "69462304-2c43-426a-e8cc-24d83db43540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SISTEM PREDIKSI TWEET GUNUNG MELETUS\n",
            "============================================================\n",
            "Model berhasil dimuat\n",
            "\n",
            "----------------------------------------\n",
            "MENU PREDIKSI:\n",
            "1. Input tweet manual\n",
            "2. Upload file CSV\n",
            "3. Keluar\n",
            "----------------------------------------\n",
            "Pilih menu (1/2/3): 1\n",
            "\n",
            "INPUT TWEET MANUAL\n",
            "========================================\n",
            "Masukkan teks tweet: Brandon Stanton jadi sosok di balik Humans of New York, sebuah proyek fotografi yang membagikan kisah-kisah menarik dari ribuan orang yang ia temui.   Kepada Mata Najwa, Brandon bercerita soal asal mula Humans of New York, impak-impak yang terbangun karenanya, hingga karya-karyanya yang lain seperti film dokumenter, buku, hingga pameran.   Sudah tayang! Sosok di Balik Humans of New York: Pemburu Ribuan Wajah tayang di YouTube Najwa Shihab dan http://narasi.tv  | Mata Najwa  #MataNajwa #HumansofNewYork #Narasi #JadiPaham\n",
            "\n",
            "HASIL PREDIKSI:\n",
            "   Tweet: Brandon Stanton jadi sosok di balik Humans of New York, sebuah proyek fotografi yang membagikan kisah-kisah menarik dari ribuan orang yang ia temui.   Kepada Mata Najwa, Brandon bercerita soal asal mula Humans of New York, impak-impak yang terbangun karenanya, hingga karya-karyanya yang lain seperti film dokumenter, buku, hingga pameran.   Sudah tayang! Sosok di Balik Humans of New York: Pemburu Ribuan Wajah tayang di YouTube Najwa Shihab dan http://narasi.tv  | Mata Najwa  #MataNajwa #HumansofNewYork #Narasi #JadiPaham\n",
            "   Kategori: BUKAN gunung meletus\n",
            "   Probabilitas gunung meletus: 45.57%\n",
            "   Probabilitas bukan: 54.43%\n",
            "\n",
            "----------------------------------------\n",
            "MENU PREDIKSI:\n",
            "1. Input tweet manual\n",
            "2. Upload file CSV\n",
            "3. Keluar\n",
            "----------------------------------------\n",
            "Pilih menu (1/2/3): 1\n",
            "\n",
            "INPUT TWEET MANUAL\n",
            "========================================\n"
          ]
        }
      ]
    }
  ]
}